{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "9r332WP2lPhs",
        "outputId": "7af19c3f-ec19-4ad4-a6dd-9e6b7feb99b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/cyclegan/"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/cyclegan\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAEvAV9Dkq9Q",
        "outputId": "4b4097b1-0c60-4d2e-d8a8-447e4cbc8a2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate http://homepages.cs.ncl.ac.uk/stephen.mcgough/Teaching/PubFig.zip\n",
        "!wget --no-check-certificate http://homepages.cs.ncl.ac.uk/stephen.mcgough/Teaching/catsNdogs.zip"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-20 18:59:56--  http://homepages.cs.ncl.ac.uk/stephen.mcgough/Teaching/PubFig.zip\n",
            "Resolving homepages.cs.ncl.ac.uk (homepages.cs.ncl.ac.uk)... 128.240.212.24\n",
            "Connecting to homepages.cs.ncl.ac.uk (homepages.cs.ncl.ac.uk)|128.240.212.24|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 184270526 (176M) [application/zip]\n",
            "Saving to: ‘PubFig.zip’\n",
            "\n",
            "PubFig.zip          100%[===================>] 175.73M  7.06MB/s    in 25s     \n",
            "\n",
            "2022-08-20 19:00:21 (7.06 MB/s) - ‘PubFig.zip’ saved [184270526/184270526]\n",
            "\n",
            "--2022-08-20 19:00:21--  http://homepages.cs.ncl.ac.uk/stephen.mcgough/Teaching/catsNdogs.zip\n",
            "Resolving homepages.cs.ncl.ac.uk (homepages.cs.ncl.ac.uk)... 128.240.212.24\n",
            "Connecting to homepages.cs.ncl.ac.uk (homepages.cs.ncl.ac.uk)|128.240.212.24|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3796145485 (3.5G) [application/zip]\n",
            "Saving to: ‘catsNdogs.zip’\n",
            "\n",
            "catsNdogs.zip       100%[===================>]   3.54G  6.45MB/s    in 9m 29s  \n",
            "\n",
            "2022-08-20 19:09:50 (6.36 MB/s) - ‘catsNdogs.zip’ saved [3796145485/3796145485]\n",
            "\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTTF6KUwkq9T",
        "outputId": "46e852c5-91de-44df-ee81-41f21681dc1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_file = '/content/cyclegan/PubFig.zip'\n",
        "zip_ref = zipfile.ZipFile(zip_file, 'r')\n",
        "zip_ref.extractall('/content/cyclegan/')\n",
        "zip_ref.close()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1645463225896
        },
        "id": "b1LY2PvDkq9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zip_file = '/content/cyclegan/catsNdogs.zip'\n",
        "zip_ref = zipfile.ZipFile(zip_file, 'r')\n",
        "zip_ref.extractall('/content/cyclegan')\n",
        "zip_ref.close()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1645463461327
        },
        "id": "gs2GA4Kukq9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U tensorflow-addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTB_NhVftqRs",
        "outputId": "582cbdde-0046-405a-ad30-1ac6c6887dc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.17.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (21.3)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys, shutil\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "autotune = tf.data.AUTOTUNE"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1645705050499
        },
        "id": "QuOTUgjIkq9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "workingdir = os.path.abspath('')\n",
        "humandir = os.path.join(workingdir + '/CelebDataProcessed/')\n",
        "catsanddogsdir = os.path.join(workingdir + '/catsNdogs/dogs vs cats/')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1645705050612
        },
        "id": "2dU2-0aMkq9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "humandatasettrain = tf.keras.preprocessing.image_dataset_from_directory(humandir, batch_size = 1, \n",
        "                    image_size = (100,100), subset = 'training', validation_split = 0.2, seed = 42)\n",
        "\n",
        "humandatasettest = tf.keras.preprocessing.image_dataset_from_directory(humandir, batch_size = 1,\n",
        "                    image_size = (100,100), subset = 'validation', validation_split = 0.2, seed = 42)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 11640 files belonging to 150 classes.\n",
            "Using 9312 files for training.\n",
            "Found 11640 files belonging to 150 classes.\n",
            "Using 2328 files for validation.\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1645705075197
        },
        "id": "GUSEvj72kq9V",
        "outputId": "6ca7c95a-69db-4aa3-c31e-cf54626663ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "catanddogdatasettrain = tf.keras.preprocessing.image_dataset_from_directory(catsanddogsdir, batch_size = 1, \n",
        "                        image_size = (100,100), subset = 'training', validation_split = 0.2, seed = 42)\n",
        "\n",
        "catanddogdatasettest = tf.keras.preprocessing.image_dataset_from_directory(catsanddogsdir, batch_size = 1,\n",
        "                        image_size = (100,100), subset = 'validation', validation_split = 0.2, seed = 42)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 37500 files belonging to 2 classes.\n",
            "Using 30000 files for training.\n",
            "Found 37500 files belonging to 2 classes.\n",
            "Using 7500 files for validation.\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1645705076660
        },
        "id": "FP03BYdNkq9V",
        "outputId": "06460cbd-6d45-4574-efd5-0d83c57881c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the standard image size.\n",
        "orig_img_size = (100, 100)\n",
        "# Size of the random crops to be used during training.\n",
        "input_img_size = (100, 100, 3)\n",
        "# Weights initializer for the layers.\n",
        "kernel_init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "# Gamma initializer for instance normalization.\n",
        "gamma_init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "buffer_size = 256\n",
        "batch_size = 1\n",
        "\n",
        "def normalize_img(img):\n",
        "    img = tf.cast(img, dtype=tf.float32)\n",
        "    # Map values in the range [-1, 1]\n",
        "    return (img / 127.5) - 1.0\n",
        "\n",
        "def preprocess_train_image(img, label):\n",
        "    # Random flip\n",
        "    img = tf.image.random_flip_left_right(img)\n",
        "    # Resize to the original size first\n",
        "    img = tf.image.resize(img, [*orig_img_size])\n",
        "    # Random crop to 256X256\n",
        "    #img = tf.image.random_crop(img, size=[*input_img_size])\n",
        "    # Normalize the pixel values in the range [-1, 1]\n",
        "    img = normalize_img(img)\n",
        "    return img\n",
        "\n",
        "def preprocess_test_image(img, label):\n",
        "    # Only resizing and normalization for the test images.\n",
        "    img = tf.image.resize(img, [input_img_size[0], input_img_size[1]])\n",
        "    img = normalize_img(img)\n",
        "    return img"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1645705076837
        },
        "id": "nqlYS5Lukq9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the preprocessing operations to the training data\n",
        "trainhuman = (\n",
        "    humandatasettrain.map(preprocess_train_image, num_parallel_calls=autotune)\n",
        "    .cache()\n",
        "    .shuffle(buffer_size)\n",
        ")\n",
        "traincatanddog = (\n",
        "    catanddogdatasettrain.map(preprocess_train_image, num_parallel_calls=autotune)\n",
        "    .cache()\n",
        "    .shuffle(buffer_size)\n",
        ")\n",
        "\n",
        "# Apply the preprocessing operations to the test data\n",
        "testhuman = (\n",
        "    humandatasettest.map(preprocess_test_image, num_parallel_calls=autotune)\n",
        "    .cache()\n",
        "    .shuffle(buffer_size)\n",
        ")\n",
        "testcatanddog = (\n",
        "    catanddogdatasettest.map(preprocess_test_image, num_parallel_calls=autotune)\n",
        "    .cache()\n",
        "    .shuffle(buffer_size)\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1645705076974
        },
        "id": "YLqHO0J8kq9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_, ax = plt.subplots(4, 2, figsize=(10, 15))\n",
        "for i, samples in enumerate(zip(trainhuman.take(4), traincatanddog.take(4))):\n",
        "    horse = (((samples[0][0] * 127.5) + 127.5).numpy()).astype(np.uint8)\n",
        "    zebra = (((samples[1][0] * 127.5) + 127.5).numpy()).astype(np.uint8)\n",
        "    ax[i, 0].imshow(horse)\n",
        "    ax[i, 1].imshow(zebra)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1645705088369
        },
        "id": "vYlXB_NRkq9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReflectionPadding2D(layers.Layer):\n",
        "    \"\"\"Implements Reflection Padding as a layer.\n",
        "\n",
        "    Args:\n",
        "        padding(tuple): Amount of padding for the\n",
        "        spatial dimensions.\n",
        "\n",
        "    Returns:\n",
        "        A padded tensor with the same type as the input tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, padding=(1, 1), **kwargs):\n",
        "        self.padding = tuple(padding)\n",
        "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, input_tensor, mask=None):\n",
        "        padding_width, padding_height = self.padding\n",
        "        padding_tensor = [\n",
        "            [0, 0],\n",
        "            [padding_height, padding_height],\n",
        "            [padding_width, padding_width],\n",
        "            [0, 0],\n",
        "        ]\n",
        "        return tf.pad(input_tensor, padding_tensor, mode=\"REFLECT\")\n",
        "\n",
        "\n",
        "def residual_block(\n",
        "    x,\n",
        "    activation,\n",
        "    kernel_initializer=kernel_init,\n",
        "    kernel_size=(3, 3),\n",
        "    strides=(1, 1),\n",
        "    padding=\"valid\",\n",
        "    gamma_initializer=gamma_init,\n",
        "    use_bias=False,\n",
        "):\n",
        "    dim = x.shape[-1]\n",
        "    input_tensor = x\n",
        "\n",
        "    x = ReflectionPadding2D()(input_tensor)\n",
        "    x = layers.Conv2D(\n",
        "        dim,\n",
        "        kernel_size,\n",
        "        strides=strides,\n",
        "        kernel_initializer=kernel_initializer,\n",
        "        padding=padding,\n",
        "        use_bias=use_bias,\n",
        "    )(x)\n",
        "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
        "    x = activation(x)\n",
        "\n",
        "    x = ReflectionPadding2D()(x)\n",
        "    x = layers.Conv2D(\n",
        "        dim,\n",
        "        kernel_size,\n",
        "        strides=strides,\n",
        "        kernel_initializer=kernel_initializer,\n",
        "        padding=padding,\n",
        "        use_bias=use_bias,\n",
        "    )(x)\n",
        "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
        "    x = layers.add([input_tensor, x])\n",
        "    return x\n",
        "\n",
        "\n",
        "def downsample(\n",
        "    x,\n",
        "    filters,\n",
        "    activation,\n",
        "    kernel_initializer=kernel_init,\n",
        "    kernel_size=(3, 3),\n",
        "    strides=(2, 2),\n",
        "    padding=\"same\",\n",
        "    gamma_initializer=gamma_init,\n",
        "    use_bias=False,\n",
        "):\n",
        "    x = layers.Conv2D(\n",
        "        filters,\n",
        "        kernel_size,\n",
        "        strides=strides,\n",
        "        kernel_initializer=kernel_initializer,\n",
        "        padding=padding,\n",
        "        use_bias=use_bias,\n",
        "    )(x)\n",
        "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
        "    if activation:\n",
        "        x = activation(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def upsample(\n",
        "    x,\n",
        "    filters,\n",
        "    activation,\n",
        "    kernel_size=(3, 3),\n",
        "    strides=(2, 2),\n",
        "    padding=\"same\",\n",
        "    kernel_initializer=kernel_init,\n",
        "    gamma_initializer=gamma_init,\n",
        "    use_bias=False,\n",
        "):\n",
        "    x = layers.Conv2DTranspose(\n",
        "        filters,\n",
        "        kernel_size,\n",
        "        strides=strides,\n",
        "        padding=padding,\n",
        "        kernel_initializer=kernel_initializer,\n",
        "        use_bias=use_bias,\n",
        "    )(x)\n",
        "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
        "    if activation:\n",
        "        x = activation(x)\n",
        "    return x"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1645705088586
        },
        "id": "_QKxOqwBkq9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_resnet_generator(\n",
        "    filters=64,\n",
        "    num_downsampling_blocks=2,\n",
        "    num_residual_blocks=9,\n",
        "    num_upsample_blocks=2,\n",
        "    gamma_initializer=gamma_init,\n",
        "    name=None,\n",
        "):\n",
        "    img_input = layers.Input(shape=input_img_size, name=name + \"_img_input\")\n",
        "    x = ReflectionPadding2D(padding=(3, 3))(img_input)\n",
        "    x = layers.Conv2D(filters, (7, 7), kernel_initializer=kernel_init, use_bias=False)(\n",
        "        x\n",
        "    )\n",
        "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    # Downsampling\n",
        "    for _ in range(num_downsampling_blocks):\n",
        "        filters *= 2\n",
        "        x = downsample(x, filters=filters, activation=layers.Activation(\"relu\"))\n",
        "\n",
        "    # Residual blocks\n",
        "    for _ in range(num_residual_blocks):\n",
        "        x = residual_block(x, activation=layers.Activation(\"relu\"))\n",
        "\n",
        "    # Upsampling\n",
        "    for _ in range(num_upsample_blocks):\n",
        "        filters //= 2\n",
        "        x = upsample(x, filters, activation=layers.Activation(\"relu\"))\n",
        "\n",
        "    # Final block\n",
        "    x = ReflectionPadding2D(padding=(3, 3))(x)\n",
        "    x = layers.Conv2D(3, (7, 7), padding=\"valid\")(x)\n",
        "    x = layers.Activation(\"tanh\")(x)\n",
        "\n",
        "    model = tf.keras.models.Model(img_input, x, name=name)\n",
        "    return model"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1645705088676
        },
        "id": "e25kcWZykq9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_discriminator(\n",
        "    filters=64, kernel_initializer=kernel_init, num_downsampling=3, name=None\n",
        "):\n",
        "    img_input = layers.Input(shape=input_img_size, name=name + \"_img_input\")\n",
        "    x = layers.Conv2D(\n",
        "        filters,\n",
        "        (4, 4),\n",
        "        strides=(2, 2),\n",
        "        padding=\"same\",\n",
        "        kernel_initializer=kernel_initializer,\n",
        "    )(img_input)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    num_filters = filters\n",
        "    for num_downsample_block in range(3):\n",
        "        num_filters *= 2\n",
        "        if num_downsample_block < 2:\n",
        "            x = downsample(\n",
        "                x,\n",
        "                filters=num_filters,\n",
        "                activation=layers.LeakyReLU(0.2),\n",
        "                kernel_size=(4, 4),\n",
        "                strides=(2, 2),\n",
        "            )\n",
        "        else:\n",
        "            x = downsample(\n",
        "                x,\n",
        "                filters=num_filters,\n",
        "                activation=layers.LeakyReLU(0.2),\n",
        "                kernel_size=(4, 4),\n",
        "                strides=(1, 1),\n",
        "            )\n",
        "\n",
        "    x = layers.Conv2D(\n",
        "        1, (4, 4), strides=(1, 1), padding=\"same\", kernel_initializer=kernel_initializer\n",
        "    )(x)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=img_input, outputs=x, name=name)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Get the generators\n",
        "gen_G = get_resnet_generator(name=\"generator_G\")\n",
        "gen_F = get_resnet_generator(name=\"generator_F\")\n",
        "\n",
        "# Get the discriminators\n",
        "disc_X = get_discriminator(name=\"discriminator_X\")\n",
        "disc_Y = get_discriminator(name=\"discriminator_Y\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1645705089574
        },
        "id": "ukLqPGeSkq9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CycleGan(tf.keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        generator_G,\n",
        "        generator_F,\n",
        "        discriminator_X,\n",
        "        discriminator_Y,\n",
        "        lambda_cycle=10.0,\n",
        "        lambda_identity=0.5\n",
        "    ):\n",
        "        super(CycleGan, self).__init__()\n",
        "        self.gen_G = generator_G\n",
        "        self.gen_F = generator_F\n",
        "        self.disc_X = discriminator_X\n",
        "        self.disc_Y = discriminator_Y\n",
        "        self.lambda_cycle = lambda_cycle\n",
        "        self.lambda_identity = lambda_identity\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.w = self.add_weight(\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.b = self.add_weight(\n",
        "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs, self.w) + self.b\n",
        "\n",
        "    def compile(\n",
        "        self,\n",
        "        gen_G_optimizer,\n",
        "        gen_F_optimizer,\n",
        "        disc_X_optimizer,\n",
        "        disc_Y_optimizer,\n",
        "        gen_loss_fn,\n",
        "        disc_loss_fn,\n",
        "    ):\n",
        "        super(CycleGan, self).compile()\n",
        "        self.gen_G_optimizer = gen_G_optimizer\n",
        "        self.gen_F_optimizer = gen_F_optimizer\n",
        "        self.disc_X_optimizer = disc_X_optimizer\n",
        "        self.disc_Y_optimizer = disc_Y_optimizer\n",
        "        self.generator_loss_fn = gen_loss_fn\n",
        "        self.discriminator_loss_fn = disc_loss_fn\n",
        "        self.cycle_loss_fn = tf.keras.losses.MeanAbsoluteError()\n",
        "        self.identity_loss_fn = tf.keras.losses.MeanAbsoluteError()\n",
        "\n",
        "    def train_step(self, batch_data):\n",
        "        # x is human and y is cats/dogs\n",
        "        real_x, real_y = batch_data\n",
        "\n",
        "        # For CycleGAN, we need to calculate different\n",
        "        # kinds of losses for the generators and discriminators.\n",
        "        # We will perform the following steps here:\n",
        "        #\n",
        "        # 1. Pass real images through the generators and get the generated images\n",
        "        # 2. Pass the generated images back to the generators to check if we\n",
        "        #    we can predict the original image from the generated image.\n",
        "        # 3. Do an identity mapping of the real images using the generators.\n",
        "        # 4. Pass the generated images in 1) to the corresponding discriminators.\n",
        "        # 5. Calculate the generators total loss (adverserial + cycle + identity)\n",
        "        # 6. Calculate the discriminators loss\n",
        "        # 7. Update the weights of the generators\n",
        "        # 8. Update the weights of the discriminators\n",
        "        # 9. Return the losses in a dictionary\n",
        "\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            # Human to fake cat/dog\n",
        "            fake_y = self.gen_G(real_x, training=True)\n",
        "            # Cat/dog to fake human -> y2x\n",
        "            fake_x = self.gen_F(real_y, training=True)\n",
        "\n",
        "            # Cycle (Human to fake cat/dog to fake horse): x -> y -> x\n",
        "            cycled_x = self.gen_F(fake_y, training=True)\n",
        "            # Cycle (Cat/dog to fake human to fake cat/dog) y -> x -> y\n",
        "            cycled_y = self.gen_G(fake_x, training=True)\n",
        "\n",
        "            # Identity mapping\n",
        "            same_x = self.gen_F(real_x, training=True)\n",
        "            same_y = self.gen_G(real_y, training=True)\n",
        "\n",
        "            # Discriminator output\n",
        "            disc_real_x = self.disc_X(real_x, training=True)\n",
        "            disc_fake_x = self.disc_X(fake_x, training=True)\n",
        "\n",
        "            disc_real_y = self.disc_Y(real_y, training=True)\n",
        "            disc_fake_y = self.disc_Y(fake_y, training=True)\n",
        "\n",
        "            # Generator adverserial loss\n",
        "            gen_G_loss = self.generator_loss_fn(disc_fake_y)\n",
        "            gen_F_loss = self.generator_loss_fn(disc_fake_x)\n",
        "\n",
        "            # Generator cycle loss\n",
        "            cycle_loss_G = self.cycle_loss_fn(real_y, cycled_y) * self.lambda_cycle\n",
        "            cycle_loss_F = self.cycle_loss_fn(real_x, cycled_x) * self.lambda_cycle\n",
        "\n",
        "            # Generator identity loss\n",
        "            id_loss_G = (\n",
        "                self.identity_loss_fn(real_y, same_y)\n",
        "                * self.lambda_cycle\n",
        "                * self.lambda_identity\n",
        "            )\n",
        "            id_loss_F = (\n",
        "                self.identity_loss_fn(real_x, same_x)\n",
        "                * self.lambda_cycle\n",
        "                * self.lambda_identity\n",
        "            )\n",
        "\n",
        "            # Total generator loss\n",
        "            total_loss_G = gen_G_loss + cycle_loss_G + id_loss_G\n",
        "            total_loss_F = gen_F_loss + cycle_loss_F + id_loss_F\n",
        "\n",
        "            # Discriminator loss\n",
        "            disc_X_loss = self.discriminator_loss_fn(disc_real_x, disc_fake_x)\n",
        "            disc_Y_loss = self.discriminator_loss_fn(disc_real_y, disc_fake_y)\n",
        "\n",
        "        # Get the gradients for the generators\n",
        "        grads_G = tape.gradient(total_loss_G, self.gen_G.trainable_variables)\n",
        "        grads_F = tape.gradient(total_loss_F, self.gen_F.trainable_variables)\n",
        "\n",
        "        # Get the gradients for the discriminators\n",
        "        disc_X_grads = tape.gradient(disc_X_loss, self.disc_X.trainable_variables)\n",
        "        disc_Y_grads = tape.gradient(disc_Y_loss, self.disc_Y.trainable_variables)\n",
        "\n",
        "        # Update the weights of the generators\n",
        "        self.gen_G_optimizer.apply_gradients(\n",
        "            zip(grads_G, self.gen_G.trainable_variables)\n",
        "        )\n",
        "        self.gen_F_optimizer.apply_gradients(\n",
        "            zip(grads_F, self.gen_F.trainable_variables)\n",
        "        )\n",
        "\n",
        "        # Update the weights of the discriminators\n",
        "        self.disc_X_optimizer.apply_gradients(\n",
        "            zip(disc_X_grads, self.disc_X.trainable_variables)\n",
        "        )\n",
        "        self.disc_Y_optimizer.apply_gradients(\n",
        "            zip(disc_Y_grads, self.disc_Y.trainable_variables)\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"G_loss\": total_loss_G,\n",
        "            \"F_loss\": total_loss_F,\n",
        "            \"D_X_loss\": disc_X_loss,\n",
        "            \"D_Y_loss\": disc_Y_loss,\n",
        "        }"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1645718316671
        },
        "id": "8P89LOw8kq9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GANMonitor(tf.keras.callbacks.Callback):\n",
        "    \"\"\"A callback to generate and save images after each epoch\"\"\"\n",
        "\n",
        "    def __init__(self, num_img=4):\n",
        "        self.num_img = num_img\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        _, ax = plt.subplots(4, 2, figsize=(12, 12))\n",
        "        for i, img in enumerate(test_horses.take(self.num_img)):\n",
        "            prediction = self.model.gen_G(img)[0].numpy()\n",
        "            prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n",
        "            img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n",
        "\n",
        "            ax[i, 0].imshow(img)\n",
        "            ax[i, 1].imshow(prediction)\n",
        "            ax[i, 0].set_title(\"Input image\")\n",
        "            ax[i, 1].set_title(\"Translated image\")\n",
        "            ax[i, 0].axis(\"off\")\n",
        "            ax[i, 1].axis(\"off\")\n",
        "\n",
        "            prediction = tf.keras.preprocessing.image.array_to_img(prediction)\n",
        "            prediction.save(\n",
        "                \"generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch + 1)\n",
        "            )\n",
        "        plt.show()\n",
        "        plt.close()\n"
      ],
      "metadata": {
        "id": "0pfbIfIasLXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function for evaluating adversarial loss\n",
        "adv_loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "# Define the loss function for the generators\n",
        "def generator_loss_fn(fake):\n",
        "    fake_loss = adv_loss_fn(tf.ones_like(fake), fake)\n",
        "    return fake_loss\n",
        "\n",
        "\n",
        "# Define the loss function for the discriminators\n",
        "def discriminator_loss_fn(real, fake):\n",
        "    real_loss = adv_loss_fn(tf.ones_like(real), real)\n",
        "    fake_loss = adv_loss_fn(tf.zeros_like(fake), fake)\n",
        "    return (real_loss + fake_loss) * 0.5\n",
        "\n",
        "\n",
        "# Create cycle gan model\n",
        "cycle_gan_model = CycleGan(\n",
        "    generator_G=gen_G, generator_F=gen_F, discriminator_X=disc_X, discriminator_Y=disc_Y\n",
        ")\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "cycle_gan_model.compile(\n",
        "    gen_G_optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
        "    gen_F_optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
        "    disc_X_optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
        "    disc_Y_optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
        "    gen_loss_fn=generator_loss_fn,\n",
        "    disc_loss_fn=discriminator_loss_fn,\n",
        ")\n",
        "#Callbacks\n",
        "plotter = GANMonitor()\n",
        "checkpoint_filepath = \"./model_checkpoints/cyclegan_checkpoints.{epoch:03d}\"\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "  filepath=checkpoint_filepath\n",
        ")\n",
        "\n",
        "# Here we will train the model for just one epoch as each epoch takes around\n",
        "# 7 minutes on a single P100 backed machine.\n",
        "\n",
        "# Load the checkpoints\n",
        "# weight_file = \"./saved_checkpoints/cyclegan_checkpoints.090\"\n",
        "# cycle_gan_model.load_weights(weight_file).expect_partial()\n",
        "# print(\"Weights loaded successfully\")\n",
        "\n",
        "cycle_gan_model.fit(\n",
        "    tf.data.Dataset.zip((trainhuman, traincatanddog)),\n",
        "    epochs=100, callbacks=[plotter, model_checkpoint_callback]\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1645785760818
        },
        "id": "P3uaPXBZkq9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_, ax = plt.subplots(4, 2, figsize=(10, 15))\n",
        "for i, img in enumerate(testhuman.take(4)):\n",
        "    prediction = cycle_gan_model.gen_G(img, training=True)[0].numpy()\n",
        "    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n",
        "    img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n",
        "\n",
        "    ax[i, 0].imshow(img)\n",
        "    ax[i, 1].imshow(prediction)\n",
        "    ax[i, 0].set_title(\"Input image\")\n",
        "    ax[i, 0].set_title(\"Input image\")\n",
        "    ax[i, 1].set_title(\"Translated image\")\n",
        "    ax[i, 0].axis(\"off\")\n",
        "    ax[i, 1].axis(\"off\")\n",
        "\n",
        "    prediction = tf.keras.preprocessing.image.array_to_img(prediction)\n",
        "    prediction.save(\"predicted_img_{i}.png\".format(i=i))\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1645787255867
        },
        "id": "QkqqDb23kq9b"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "azureml_py38_pt_tf",
      "language": "python",
      "display_name": "Python 3.8 - Pytorch and Tensorflow"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.1",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "azureml_py38_pt_tf"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "background_execution": "on",
      "name": "cyclegan.ipynb",
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}